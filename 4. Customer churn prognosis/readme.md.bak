# Прогнозирование Оттока Клиентов в Банке## Описание проектаИз «Бета-Банка» стали уходить клиенты. Каждый месяц. Немного, но заметно. Банковские маркетологи посчитали: сохранять текущих клиентов дешевле, чем привлекать новых.На основе исторических данных о клиентах нужно спрогнозировать, уйдёт клиент из банка в ближайшее время или нет. ## Инструменты:`Python==3.9.16``Pandas==1.5.3``sklearn==1.2.2``matplotlib==3.6.3``numpy==1.23.5`## Описание данных:- RowNumber — индекс строки в данных- CustomerId — уникальный идентификатор клиента- Surname — фамилия- CreditScore — кредитный рейтинг- Geography — страна проживания- Gender — пол- Age — возраст- Tenure — сколько лет человек является клиентом банка- Balance — баланс на счёте- NumOfProducts — количество продуктов банка, используемых клиентом- HasCrCard — наличие кредитной карты- IsActiveMember — активность клиента- EstimatedSalary — предполагаемая зарплата## ВыводыИзучены данные об оттоке клиентов.- Проведена предобработка данных	- Изучено распределение признаков	- Обнаружены пропуски в признаке `tenure`. Не выявлено закономерноестей, позвоволяющих заполнить пропуски на основании других признаков. Ввиду малого количества пропусков (<10%) пропуски были удалены	- Удалены данные, не относящиеся к исследованию (номера строк, имена, ID)- Изучен баланс классов. Дисбаланс составляет 4:1 в пользу отрицательных классов.- Проведено разбиение выборки на обучающую и тестовую в соотношеннии 4:1. Валидационная выборка не выделялась, т. к. было решено использовать кросс-валидацию- Проведено масштабирование количественных признаков для улучшения качества линейных моделей.- Проведено первичное обучение моделей и подбор гиперпараметров без учёта дисбаланса классов. Расчитаны значения целевой метрики f1 и дополнительной - AUC-ROC. Ни одна из моделей не достигла целевого значения метрики f1	- Логистическая регрессия: f1 = 0.32, AUC-ROC = 0.77	- Дерево решений (max_depth = 6): f1 = 0.56, AUC-ROC = 0.83	- Случайный лес (max_depth = 18, n_estimators = 50): f1 = 0.58, AUC-ROC = 0.85- Опробовано три способа борьбы с дисбалансом:	- взвешивание классов:		- Логистическая регрессия: f1 = 0.5, AUC-ROC = 0.77		- Дерево решений (class_weight = 'balanced', max_depth = 6): f1 = 0.57, AUC-ROC = 0.83		- Случайный лес (max_depth = 18, n_estimators = 50): f1 = 0.62, AUC-ROC = 0.86	- upsampling:		- Логистическая регрессия: f1 = 0.74, AUC-ROC = 0.81		- Дерево решений (max_depth = 6): f1 = 0.83, AUC-ROC = 0.83		- Случайный лес (class_weight = 'balanced', max_depth = 9, n_estimators = 40): f1 = 0.89, AUC-ROC = 0.96	- downsampling:		- Логистическая регрессия: f1 = 0.72, AUC-ROC = 0.81		- Дерево решений (max_depth = 5): f1 = 0.76, AUC-ROC = 0.84		- Случайный лес (max_depth = 8, n_estimators = 40): f1 = 0.79, AUC-ROC = 0.86- Наиболее высокие резульаты во всех случаях показывает модель случайного леса.- Проведена проверка моделей случайного леса на тестовой выборке,	- модель, обученная на апсемплированной выборке, показавшая наилучный результат на кросс-валидации (f1 = 0.89) на тестовой выборке имеет f1 = 0.58, что ниже порогового значения	- модель, обученная на даунсемплированной выборке, на тестовой выборке имеет f1 = 0.59	- наилучший результат показывает модель со взвешенными классами - f1 = 0.61 на тестовой выборке.